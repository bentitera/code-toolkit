{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping\n",
    "\n",
    "To begin, we will examine the reddit page dealing with Machine Learning.  Our goal is to scrape the basic information for posts.\n",
    "\n",
    "![](images/reddit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/r/MachineLearning/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!doctype html>\\n<html>\\n  <head>\\n    <title>Too Many Requests</title>\\n    <style>\\n      body {\\n          font: small verdana, arial, helvetica, sans-serif;\\n          width: 600px;\\n          margin: 0 auto;\\n      }\\n\\n      h1 {\\n          height: 40px;\\n          background: transparent url(//www.redditstatic.com/reddit.com.header.png) no-repeat scroll top right;\\n      }\\n    </style>\\n  </head>\\n  <body>\\n    <h1>whoa there, pardner!</h1>\\n    \\n\\n\\n<p>we\\'re sorry, but you appear to be a bot and we\\'ve seen too many requests\\nfrom you lately. we enforce a hard speed limit on requests that appear to come\\nfrom bots to prevent abuse.</p>\\n\\n<p>if you are not a bot but are spoofing one via your browser\\'s user agent\\nstring: please change your user agent string to avoid seeing this message\\nagain.</p>\\n\\n<p>please wait 6 second(s) and try again.</p>\\n\\n    <p>as a reminder to developers, we recommend that clients make no\\n    more than <a href=\"http://github.com/reddit/reddit/wiki/API\">one\\n    request every two seconds</a> to avoid seeing this message.</p>\\n  </body>\\n</html>\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [429]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>This is a header</h1>\n",
       "<p>This would be a paragraph. <strong>Strong Words</strong> here. </p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h1>This is a header</h1>\n",
    "<p class = 'super-paragraph'>This would be a paragraph. <strong>Strong Words</strong> here. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_21_Jump_Street_episodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>List of 21 Jump Street episodes - Wikipedia</title>\\n<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );</script>\\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"List_of_21_Jump_Street_episodes\",\"wgTitle\":\"List of 21 Jump Street episodes\",\"wgCurRevisionId\":844038329,\"wgRevisionId\":844038329,\"wgArticleId\":35403829,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles needing additional references from May 2012\",\"All articles needing additional references\",\"21 Jump Street\",\"Lists of American crime television series episodes\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransfo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2>Contents</h2>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h2 = soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season 1 (1987)[edit]\n",
      "Season 2 (1987-88)[edit]\n",
      "Season 3 (1988-89)[edit]\n",
      "Season 4 (1989-90)[edit]\n",
      "Season 5 (1990-91)[edit]\n"
     ]
    }
   ],
   "source": [
    "for header in all_h2[2:7]:\n",
    "    print(header.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1 = soup.find('table', {'class': 'wikitable plainrowheaders'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1_titles = table_1.find_all('td', {'class': 'summary'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Pilot\"\n",
      "\"America, What a Town\"\n",
      "\"Don't Pet the Teacher\"\n",
      "\"My Future's So Bright, I Gotta Wear Shades\"\n",
      "\"The Worst Night of Your Life\"\n",
      "\"Gotta Finish the Riff\"\n",
      "\"Bad Influence\"\n",
      "\"Blindsided\"\n",
      "\"Next Generation\"\n",
      "\"Low and Away\"\"Running on Ice\"\n",
      "\"16 Blown to 35\"\n",
      "\"Mean Streets and Pastel Houses\"\n"
     ]
    }
   ],
   "source": [
    "for title in season_1_titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p><i><a href=\"/wiki/21_Jump_Street\" title=\"21 Jump Street\">21 Jump Street</a></i> is an American <a href=\"/wiki/Police_procedural\" title=\"Police procedural\">police procedural</a> <a class=\"mw-redirect\" href=\"/wiki/Crime_drama\" title=\"Crime drama\">crime drama</a> <a class=\"mw-redirect\" href=\"/wiki/Television_series\" title=\"Television series\">television series</a> that aired on the <a href=\"/wiki/Fox_Broadcasting_Company\" title=\"Fox Broadcasting Company\">Fox Network</a> and in first run syndication from April 12, 1987, to April 27, 1991, with a total of 103 <a href=\"/wiki/Episode\" title=\"Episode\">episodes</a>. The series focuses on a squad of youthful-looking undercover police officers investigating crimes in high schools, colleges, and other teenage venues.<sup class=\"reference\" id=\"cite_ref-1\"><a href=\"#cite_note-1\">[1]</a></sup>\n",
       "</p>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href = 'https://www.reddit.com/r/MachineLearning/'> The Reddit Page </a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<a href = 'https://www.reddit.com/r/MachineLearning/'> The Reddit Page </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('h2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a', {'data-click-id': 'body'})['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for i in soup.find_all('a', {'data-click-id': 'body'}):\n",
    "    url_link = 'https://www.reddit.com' + i['href']\n",
    "    links.append(url_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "titles = []\n",
    "bodys = []\n",
    "for i in soup.find_all('a', {'data-click-id': 'body'}):\n",
    "    url_link = 'https://www.reddit.com' + i['href']\n",
    "    links.append(url_link)\n",
    "    response = requests.get(url_link)\n",
    "    soup2 = BeautifulSoup(response.text, 'html.parser')\n",
    "    title = soup2.find('h2')\n",
    "    body = soup2.find_all('p')\n",
    "    titles.append(title)\n",
    "    bodys.append(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'links': links, 'title': titles, 'body': bodys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Exercise\n",
    "\n",
    "Scraping Wikipedia tables and adding information found through links.\n",
    "\n",
    "![](images/wiki_table.png)\n",
    "\n",
    "Problem:\n",
    "\n",
    "1. Create a dataframe that contains the information displayed on the Wikipedia page \"List of 2018 Albums\".\n",
    "2. What is Sub Pop releasing in 2018?\n",
    "3. Did Drake put anything out?\n",
    "4. What label is putting out the most music?  Visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_2018_albums'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('table', {'class':'wikitable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consumer_key = 'o24LbkkTsV3eVKERVYjIznnrT'\n",
    "\n",
    "consumer_secret = 'Q4yUOhDhlagNWrgwOnqzroGHI5aWqaM1MkbkkO6p9gPRhtKIYz'\n",
    "access_token = '820718295187918848-DjESel4eJhmWto48EwBrmkCBR5vthkZ'\n",
    "access_token_secret = 'fC8KyuUJoPOft2hIpCvNVf4dWj2FH5zw6IMgdcIbqNmCK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweepy\n",
    "\n",
    "- Sign into Twitter apps (https://apps.twitter.com/)\n",
    "- Create application and retrieve `consumer_key`, `consumer_secret`, `access_token`, and `access_token_secret`.  \n",
    "- Follow example below filling in your info.  For more info, see the Tweepy documentation [here](http://tweepy.readthedocs.io/en/v3.5.0/getting_started.html#introduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user('thrashermag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the pigs feet and dog coats didn't raise the red flag, the $600 in pizza definitely did the trick. No wonder theâ€¦ https://t.co/juUgD90o4m\n",
      "Style and brains eternal, we love you Phil Shao!  https://t.co/x6osDOcDFO https://t.co/BumbRgl2Kx\n",
      "The WKND crew took a U-Haul full of ramps to the desert and things got weird. They may not have located Animal Chinâ€¦ https://t.co/PqZ6jpe26p\n",
      "With a nod to Jeremy Klein, the WKND boys hit the road Hook Ups style and put together a launch-ramp-infused tour,â€¦ https://t.co/9Sw1m5TJum\n",
      "After smashing his grill, Jaws rejoins Real for a heavy handrail day with Jamie Thomas. Tyson goes for the biggestâ€¦ https://t.co/nsuPXrr7lr\n",
      "If Dustin Dollin is signing your checks you sure as hell arenâ€™t gonna turn in any soft footage. This PD Promo is haâ€¦ https://t.co/qTl5xpyBpm\n",
      "Randy Blythe looks out for his people, loves what he does and just wants his fans to know where heâ€™s coming from.â€¦ https://t.co/gHaVT50zEX\n",
      "Frog in Las Vegas, Gridlock in SF, Brent Atchley's return and more in today's episode of Skateline.â€¦ https://t.co/4BjpB74d0P\n",
      "Jamie Thomas jumps in the Real van to coach them through a day of big-ass rails and \"one more\" tries. Watch the fulâ€¦ https://t.co/jYIKFh4S5r\n",
      "No soundtrack, just the sights and sounds of two comrades creating majestic moments on their skateboards.â€¦ https://t.co/ggyNi3w1bV\n",
      "Mike Arnold ðŸŒŠ Atlantic Drift https://t.co/jLNPbYrmnC\n",
      "It's the up-grinding powers of Cole Wilson vs Zion's unstoppable ATV skills this week. Oh, and Evan Smith's unorthoâ€¦ https://t.co/iMx9axcNWS\n",
      "After trips to SF and Vegas, the Drifters elected to test their mettle atop the volcanic peaks of paradise, bringinâ€¦ https://t.co/wiH331wNbI\n",
      "SF is the promised land. Come see for yourself. Everything is better when fried...\n",
      "https://t.co/vulmaPqRac https://t.co/HXIQYQUiog\n",
      "Realâ€™s rusty nail isnâ€™t just tech tricks and precision ledgework, heâ€™s down to get weird AF! Could a KOTR win (andâ€¦ https://t.co/hcEG5F0Sb0\n",
      "All the coffins, footballs and diapers slams â€“ without all that fancy talkinâ€™ and TV stuff. Bam gets bent, Brock geâ€¦ https://t.co/pNjZ3SkygP\n",
      "The Skate Gods deployed the Skate Rock crew to Vietnam and Bali. They stay ready for anything that comes their way.â€¦ https://t.co/buTsITIl4u\n",
      "Rowan and Frecks kick it off with a rip ride around Vietnam, meeting Ishod and the boys for an island hop to Bali dâ€¦ https://t.co/QIpnVM1f5h\n",
      "Big head, bigger pop! Masonâ€™s gonna crush KOTR, unless that hockey temper takes over. https://t.co/gkkhnlKRnc https://t.co/rNCSIJnZuG\n",
      "A full-length extravaganza from the Isles of Hawaiiâ€¦ Thank you, APB! https://t.co/U7tuIidtTx https://t.co/cFxWP2kv0Q\n"
     ]
    }
   ],
   "source": [
    "for tweet in user.timeline(limit = 500):\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418430\n"
     ]
    }
   ],
   "source": [
    "print(user.followers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for tweet in user.timeline(count = 200):\n",
    "    tweets.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"If the pigs feet and dog coats didn't raise the red flag, the $600 in pizza definitely did the trick. No wonder theâ€¦ https://t.co/juUgD90o4m\",\n",
       " 'Style and brains eternal, we love you Phil Shao!  https://t.co/x6osDOcDFO https://t.co/BumbRgl2Kx',\n",
       " 'The WKND crew took a U-Haul full of ramps to the desert and things got weird. They may not have located Animal Chinâ€¦ https://t.co/PqZ6jpe26p',\n",
       " 'With a nod to Jeremy Klein, the WKND boys hit the road Hook Ups style and put together a launch-ramp-infused tour,â€¦ https://t.co/9Sw1m5TJum',\n",
       " 'After smashing his grill, Jaws rejoins Real for a heavy handrail day with Jamie Thomas. Tyson goes for the biggestâ€¦ https://t.co/nsuPXrr7lr']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Table\n",
    "\n",
    "![](images/open_table.png)\n",
    "\n",
    "Finding restaurants in New York City. (https://www.opentable.com/new-york-restaurant-listings)  Is there good Indian food in the Upper West Side?  Where?  What are people saying is good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://www.opentable.com/new-york-restaurant-listings'\n",
    "response = requests.get('https://www.yelp.com/search?find_desc=burrito&find_loc=Civic+Center%2C+Manhattan%2C+NY&ns=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Response' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-37acf3ef279d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Response' object has no attribute 'view'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n  \\n\\n            window.yPageStart = new Date().getTime();\\n\\n            var initialVisibilityState = document.webkitVisibilityState;\\n\\n                yPerfTimings = [];\\n\\n                ySitRepParams = {\"clientIP\": \"144.121.201.14\", \"datacenter\": \"us-east-1\", \"is_internal_ip\": false, \"edgeStartT'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = soup.find_all('div', {'class': 'media-block media-block--18'})\n",
    "test2 = soup.find_all('a', {'class': 'biz-name'})\n",
    "title = []\n",
    "for t in test2:\n",
    "    title.append(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jerusalem Mexican Deli Grocery',\n",
       " 'El Vez',\n",
       " 'Holi Mole',\n",
       " 'Breakroom',\n",
       " 'Burrito House',\n",
       " 'Pulqueria',\n",
       " 'Dos Toros Taqueria',\n",
       " 'Habana To-Go',\n",
       " 'New Fresco Tortillas',\n",
       " 'Oaxaca Taqueria',\n",
       " 'Luchadores']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = soup.find_all('div', {'class': 'rest-row-header'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dorcass  \n",
      " Gregg Hegmann  \n",
      " Angelinas  \n",
      " Will  \n",
      " Lemke Ports  \n",
      " Herzog  \n",
      " Sed Erdman  \n",
      " Dolore  \n",
      " Et VonRueden  \n",
      " Rerum  \n",
      " Margarett Grant  \n",
      " Columbuss  \n",
      " 45 Kirlin  \n",
      " Mews  \n",
      " Saepe Stracke  \n",
      " Recusandae Quigley  \n",
      " Noemi Glover  \n",
      " Kemmer Ports  \n",
      " 1051 Dickinson  \n",
      " Liza Murazik  \n",
      " Earum Jacobson  \n",
      " 559 Hammes  \n",
      " Sunt Wiegand  \n",
      " Delectus  \n",
      " Jacksons  \n",
      " Brants  \n",
      " Simonis  \n",
      " Titus Cremin  \n",
      " Brenden Mills  \n",
      " Pollich  \n",
      " Columbus Pfeffer  \n",
      " Mason Pike  \n",
      " 1376 Prohaska  \n",
      " Branch  \n",
      " Tempore  \n",
      " Trail  \n",
      " Et  \n",
      " Autem  \n",
      " Hermiston  \n",
      " Crest  \n",
      " Quis Marks  \n",
      " Flo Crossroad  \n",
      " Consequatur Schinner  \n",
      " Raynor  \n",
      " Damariss  \n",
      " Agloe Bar & Grill  \n",
      " Forges  \n",
      " Marcia Shoal  \n",
      " Heathcote  \n",
      " Eum Tunnel  \n",
      " Arvilla Bosco  \n",
      " Jayde Key  \n",
      " Kunze  \n",
      " Mollie Heller  \n",
      " Exercitationem Summit  \n",
      " Lindsay Reichel  \n",
      " Quasi River  \n",
      " Crawford Willms  \n",
      " Similique  \n",
      " Luciano Hansen  \n",
      " Ratione Villages  \n",
      " Distinctio Ports  \n",
      " 1023 MacGyver  \n",
      " Ex Harbors  \n",
      " Nulla  \n",
      " Rerum Mews  \n",
      " Quasi  \n",
      " Natus Torphy  \n",
      " Beulahs  \n",
      " Groves  \n",
      " Facere Waelchi  \n",
      " Enim  \n",
      " Facere  \n",
      " Ryan  \n",
      " Eras  \n",
      " Goldner  \n",
      " Ebert Port  \n",
      " Dicta Trace  \n",
      " Facilis  \n",
      " Smitham Via  \n",
      " Hallies  \n",
      " Fidel Corner  \n",
      " Deserunt Kihn  \n",
      " Reanna Wolff  \n",
      " Sit  \n",
      " Colt Hansen  \n",
      " Quibusdam Hegmann  \n",
      " Voluptas Crescent  \n",
      " Soluta Sauer  \n",
      " Jolies  \n",
      " Fall  \n",
      " Howell  \n",
      " Kari Paucek  \n",
      " Voluptatem Koch  \n",
      " Denesik  \n",
      " Bradtke Throughway  \n",
      " 1061 Stehr  \n",
      " Gardens  \n",
      " Armstrong  \n",
      " Dominic Klein  \n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    print(name.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
